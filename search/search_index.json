{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is useless_isam ? I was working on a C++ implementation of db_tutorial made by cstack as a pet project for my portfolio when I started to get hardy time on the B+Tree implementation section. After learning about B-Tree and B+Tree, I was still struggling with indexing and persistence on files. So I decided to switch to something easier and casually bumped into ISAM indexing. Why useless? Because with this project you can't insert new records. So it's a read only database. Just basic search queries and nothing else. Also, record columns are hard-coded within the source code. So neither CREAT TABLE statement type for column definition. Quite useless, don't you agree? Well, maybe not so much useless... I like writing code and I am a C++ lover, so this small and simple project is ideal to put some things I learned until now in practice. Goal, I hope, is to produce a clean and well-written code and not something like if it compile and works then it's fine, no matter how it is made . Some keywords covered by this project are database isam indexing parsing lexer paging file manipulation paging caching cmake tokens record exception handling and I hope there will be more in the future. Overvew ISAM is used to fast retrieve data from a database. So I decided to use a binary file for storing the dataset because it allows random access to records. Also, we need a dataset to populate our database. And don't forget that the database must be sorted in order to create an index file and support range selections. Let's put in order... generate a database binary file from a dataset sort the database binary file by a column generate the index binary file So I decided to archieve these points with three separate executables. csv2bin The program will use a textual dataset to build a database.bin file consists of a collection of fixed length records. Nothing more. More precisely, records are grouped in pages and, if needed, some padding will be added to the end of each page. index-gen Then the database file will be sorted by a column and rewritten. Based upont it, the program will build a sorted index binary file where each record will represent all records belonging to a single page. Actually, only single level indexing is supoorted. isam Finally, this executable will load the index and database files for support search queries and range selections. References and useful links Connor Stack. db_tutorial Bjarne Stroustrup. The C++ Programming Language (4th edition) Grust, Torsten (Summer 2013). Tree-Structured Indexing: ISAM and B+-trees Compiler Explorer: https://godbolt.org/","title":"What is useless_isam ?"},{"location":"#what-is-useless_isam","text":"I was working on a C++ implementation of db_tutorial made by cstack as a pet project for my portfolio when I started to get hardy time on the B+Tree implementation section. After learning about B-Tree and B+Tree, I was still struggling with indexing and persistence on files. So I decided to switch to something easier and casually bumped into ISAM indexing.","title":"What is useless_isam ?"},{"location":"#why-useless","text":"Because with this project you can't insert new records. So it's a read only database. Just basic search queries and nothing else. Also, record columns are hard-coded within the source code. So neither CREAT TABLE statement type for column definition. Quite useless, don't you agree?","title":"Why useless?"},{"location":"#well-maybe-not-so-much-useless","text":"I like writing code and I am a C++ lover, so this small and simple project is ideal to put some things I learned until now in practice. Goal, I hope, is to produce a clean and well-written code and not something like if it compile and works then it's fine, no matter how it is made . Some keywords covered by this project are database isam indexing parsing lexer paging file manipulation paging caching cmake tokens record exception handling and I hope there will be more in the future.","title":"Well, maybe not so much useless..."},{"location":"#overvew","text":"ISAM is used to fast retrieve data from a database. So I decided to use a binary file for storing the dataset because it allows random access to records. Also, we need a dataset to populate our database. And don't forget that the database must be sorted in order to create an index file and support range selections. Let's put in order... generate a database binary file from a dataset sort the database binary file by a column generate the index binary file So I decided to archieve these points with three separate executables.","title":"Overvew"},{"location":"#csv2bin","text":"The program will use a textual dataset to build a database.bin file consists of a collection of fixed length records. Nothing more. More precisely, records are grouped in pages and, if needed, some padding will be added to the end of each page.","title":"csv2bin"},{"location":"#index-gen","text":"Then the database file will be sorted by a column and rewritten. Based upont it, the program will build a sorted index binary file where each record will represent all records belonging to a single page. Actually, only single level indexing is supoorted.","title":"index-gen"},{"location":"#isam","text":"Finally, this executable will load the index and database files for support search queries and range selections.","title":"isam"},{"location":"#references-and-useful-links","text":"Connor Stack. db_tutorial Bjarne Stroustrup. The C++ Programming Language (4th edition) Grust, Torsten (Summer 2013). Tree-Structured Indexing: ISAM and B+-trees Compiler Explorer: https://godbolt.org/","title":"References and useful links"},{"location":"part1/","text":"Part 1 The dataset First thing to do is to select a dataset and, in case, adapt it for our purpose. After thinking a little about it, I endend up with a banal world cities dataset. I downloaded it somewhere on the net (don't remember) and made some adjustments like columns removal and representation edits. The cities.csv dataset consists of 140.895 rows separated by newline. Each row consists of 9 columns separated by semicolon. In dept Position Name Type Min/max/length C++ type C++ size 1 Geoname ID Positive integer max: 12.514.312 int 4 2 ASCII Name String max length: 69 char[70] 70 3 Country Code 2 character string fixed length: 2 char[3] 3 4 Country Name String max length: 38 char[39] 39 5 Population Positive integer max: 22.315.474 int 4 6 DEM Signed integer [-9.999, 5.622] int 4 7 Timezone String max length: 30 char[31] 31 8 Latitude String max length: 8 char[9] 9 9 Longitude String max length: 8 char[9] 9 Total 173 bytes Instead of storing a positive integer to a unsigned variable, we can safely use a int type beacause 1 INT_MAX = 2.14.748.364 (on my machine) is less than the max possible value for Geoname ID and Population attributes. You may think that the total size of a row is 173 bytes but you can't be sure until you check the output of sizeof() operator. Let's use Compiler Explorer (using x86-64 gcc 12.2) #include <iostream> struct Record { int id; char name[70]; char country_code[3]; char country_name[39]; int population; int dem; char timezone[31]; char latitude[9]; char longitude[9]; }; int main() { std::cout << sizeof(Record) << \"\\n\"; } Output: Program returned: 0 Program stdout 176 Why sizeof(Record) returns 176? According to Stroustrup 2 : An object doesn\u2019t just need enough storage to hold its representation. In addition, on some machine architectures, the bytes used to hold it must have proper alignment for the hardware to access it efficiently (or in extreme cases to access it at all). For example, a 4-byte int often has to be aligned on a word (4-byte) boundary, and sometimes an 8-byte double has to be aligned on a word (8-byte) boundary. Of course, this is all very implementation specific, and for most programmers completely implicit. You can write good C++ code for decades without needing to be explicit about alignment. Where alignment most often becomes visible is in object layouts: sometimes struct s contain \u2018\u2018holes\u2019\u2019 to improve alignment (\u00a78.2.1). The sum of a struct (or class ) attribute members is't always equal to the in-memory size of due to alignment optimizations that are machine-dependent. So you have to check out the previouse code with your compiler. Also, even though we are using fundamental types, their size are still machine-dependent 3 . In fact: Some of the aspects of C++\u2019s fundamental types, such as the size of an int , are implementation-defined. Similarly, the int type is supposed to be chosen to be the most suitable for holding and manipulating integers on a given computer; it is typically a 4-byte (32-bit) word. It is unwise to assume more. For example, there are machines with 32-bit char s. This is interesting but also quite problematic. You may experience different sizes. If we want to be strict, we should use int32_t instead of int (including the <cstdint> header) and char8_t instead of char (using the -std=c++20 compiler flag). But this problem will not be addressed now. Since most systems are similar to my machine (a typical laptop with Intel cpu) we will assume (wrongly!) that data types and sizes are those point out in the previous table. record.h Now we can discuss some code. Let's take a look at record.h struct Record { int id; char name[70]; char country_code[3]; char country_name[39]; int population; int dem; char timezone[31]; char latitude[9]; char longitude[9]; Record() = default; Record(const Record&); Record& operator=(const Record&); }; Surely we need a copy constructor and an assignent operator . Default copy costructor will copy variables and pointers and this would be really bad. In this example, the assignment operator is almost identical to copy constructor because the struct does not holds resource like dynamic memory. Record::Record(const Record& copy): id{copy.id}, population{copy.population}, dem{copy.dem} { strcpy(name, copy.name); strcpy(country_code, copy.country_code); strcpy(country_name, copy.country_name); strcpy(timezone, copy.timezone); strcpy(latitude, copy.latitude); strcpy(longitude, copy.longitude); } Record& Record::operator=(const Record& copy) { if(this != &copy) { //Protection againts self-assignment id = copy.id; strcpy(name, copy.name); strcpy(country_code, copy.country_code); strcpy(country_name, copy.country_name); population = copy.population; dem = copy.dem; strcpy(timezone, copy.timezone); strcpy(latitude, copy.latitude); strcpy(longitude, copy.longitude); } return *this; } No need to define a custom default constructor . The one generated by the compiler is fine. But since we defined a custom copy constructor, the generation of default constructor is suppressed. So we need to get it back using =default . Remember that you can possibly initialize the struct using the {} notation. A summary about default operations generation rules can be found at the Marius Bancila's Blog . https://en.cppreference.com/w/cpp/types/climits \u21a9 from The C++ Programming Language, 4th edition, \u00a76.2.9 \u21a9 from The C++ Programming Language, 4th edition, \u00a76.2.8 \u21a9","title":"Part 1"},{"location":"part1/#part-1","text":"","title":"Part 1"},{"location":"part1/#the-dataset","text":"First thing to do is to select a dataset and, in case, adapt it for our purpose. After thinking a little about it, I endend up with a banal world cities dataset. I downloaded it somewhere on the net (don't remember) and made some adjustments like columns removal and representation edits. The cities.csv dataset consists of 140.895 rows separated by newline. Each row consists of 9 columns separated by semicolon. In dept Position Name Type Min/max/length C++ type C++ size 1 Geoname ID Positive integer max: 12.514.312 int 4 2 ASCII Name String max length: 69 char[70] 70 3 Country Code 2 character string fixed length: 2 char[3] 3 4 Country Name String max length: 38 char[39] 39 5 Population Positive integer max: 22.315.474 int 4 6 DEM Signed integer [-9.999, 5.622] int 4 7 Timezone String max length: 30 char[31] 31 8 Latitude String max length: 8 char[9] 9 9 Longitude String max length: 8 char[9] 9 Total 173 bytes Instead of storing a positive integer to a unsigned variable, we can safely use a int type beacause 1 INT_MAX = 2.14.748.364 (on my machine) is less than the max possible value for Geoname ID and Population attributes. You may think that the total size of a row is 173 bytes but you can't be sure until you check the output of sizeof() operator. Let's use Compiler Explorer (using x86-64 gcc 12.2) #include <iostream> struct Record { int id; char name[70]; char country_code[3]; char country_name[39]; int population; int dem; char timezone[31]; char latitude[9]; char longitude[9]; }; int main() { std::cout << sizeof(Record) << \"\\n\"; } Output: Program returned: 0 Program stdout 176 Why sizeof(Record) returns 176? According to Stroustrup 2 : An object doesn\u2019t just need enough storage to hold its representation. In addition, on some machine architectures, the bytes used to hold it must have proper alignment for the hardware to access it efficiently (or in extreme cases to access it at all). For example, a 4-byte int often has to be aligned on a word (4-byte) boundary, and sometimes an 8-byte double has to be aligned on a word (8-byte) boundary. Of course, this is all very implementation specific, and for most programmers completely implicit. You can write good C++ code for decades without needing to be explicit about alignment. Where alignment most often becomes visible is in object layouts: sometimes struct s contain \u2018\u2018holes\u2019\u2019 to improve alignment (\u00a78.2.1). The sum of a struct (or class ) attribute members is't always equal to the in-memory size of due to alignment optimizations that are machine-dependent. So you have to check out the previouse code with your compiler. Also, even though we are using fundamental types, their size are still machine-dependent 3 . In fact: Some of the aspects of C++\u2019s fundamental types, such as the size of an int , are implementation-defined. Similarly, the int type is supposed to be chosen to be the most suitable for holding and manipulating integers on a given computer; it is typically a 4-byte (32-bit) word. It is unwise to assume more. For example, there are machines with 32-bit char s. This is interesting but also quite problematic. You may experience different sizes. If we want to be strict, we should use int32_t instead of int (including the <cstdint> header) and char8_t instead of char (using the -std=c++20 compiler flag). But this problem will not be addressed now. Since most systems are similar to my machine (a typical laptop with Intel cpu) we will assume (wrongly!) that data types and sizes are those point out in the previous table.","title":"The dataset"},{"location":"part1/#recordh","text":"Now we can discuss some code. Let's take a look at record.h struct Record { int id; char name[70]; char country_code[3]; char country_name[39]; int population; int dem; char timezone[31]; char latitude[9]; char longitude[9]; Record() = default; Record(const Record&); Record& operator=(const Record&); }; Surely we need a copy constructor and an assignent operator . Default copy costructor will copy variables and pointers and this would be really bad. In this example, the assignment operator is almost identical to copy constructor because the struct does not holds resource like dynamic memory. Record::Record(const Record& copy): id{copy.id}, population{copy.population}, dem{copy.dem} { strcpy(name, copy.name); strcpy(country_code, copy.country_code); strcpy(country_name, copy.country_name); strcpy(timezone, copy.timezone); strcpy(latitude, copy.latitude); strcpy(longitude, copy.longitude); } Record& Record::operator=(const Record& copy) { if(this != &copy) { //Protection againts self-assignment id = copy.id; strcpy(name, copy.name); strcpy(country_code, copy.country_code); strcpy(country_name, copy.country_name); population = copy.population; dem = copy.dem; strcpy(timezone, copy.timezone); strcpy(latitude, copy.latitude); strcpy(longitude, copy.longitude); } return *this; } No need to define a custom default constructor . The one generated by the compiler is fine. But since we defined a custom copy constructor, the generation of default constructor is suppressed. So we need to get it back using =default . Remember that you can possibly initialize the struct using the {} notation. A summary about default operations generation rules can be found at the Marius Bancila's Blog . https://en.cppreference.com/w/cpp/types/climits \u21a9 from The C++ Programming Language, 4th edition, \u00a76.2.9 \u21a9 from The C++ Programming Language, 4th edition, \u00a76.2.8 \u21a9","title":"record.h"},{"location":"part2/","text":"Part 2 We need to take the csv dataset cities.csv as input and generate a binary file. We should call it databse.bin . Next, we will use this one with index-gen executable in order to generate an index binary file index.bin . For now, it's bettere to save to a file all those filenames. Let's call it params.h : #define DATASET_FILENAME \"../cities.csv\" #define DATABASE_FILENAME \"../database.bin\" #define INDEX_FILENAME \"../index.bin\" I am using CMake with Visual Studio Code. In doing so, it will be faster to compile and execute the code during the development. Organization of database file We would read all records from cities.csv and write them to database.bin like a simply collection of Record s. But we want to take advantage of paging to improve performance. This will introduce just a bit of complexity. First, add to params.h the line #define PAGE_SIZE 4096 This is usually the page size in most systems. So, if sizeof(Record) = 176 bytes, and PAGE_SIZE is 4096, each paga will contains PAGE_SIZE / sizeof(Record) = 23 records with PAGE_SIZE % sizeof(Record) = 48 overhead byte remaining. csv2bin The source csv2bin.cpp simply reads each row from DATASET_FILENAME associated input stream, save it in a Record variable and append it to DATABASE_FILENAME associated output stream. Each 23 reads we have to write 48 byte to finish a page so const unsigned records_per_page{ PAGE_SIZE / sizeof(Record) }; //23 const unsigned overhead_byte_length{ PAGE_SIZE % sizeof(Record) }; //48 byte const char overhead_data[overhead_byte_length]{0}; size_t count{0}; //Record reads counter while(!input_file.eof()) { std::getline(input_file, row, '\\n'); //extracts a row /** * Save a row/line to Record variable */ output_file.write(reinterpret_cast<char*>(&record), sizeof(Record)); count += 1; if( (count % records_per_page) == 0) { //each 23 reads output_file.write(overhead_data, overhead_byte_length); //finish a page } } //end while-loop And this is the core idea. Extracting attributes from a row Each column is separate by a ; delimiter and we are storing an entire row to a std::string class type. So let's take advantage from the C++ standard library using find() 1 and substr() 2 methods. We could do something like this for int type std::string attr; next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); column = strtoul(attr.c_str(), nullptr, 10); last_pos = next_pos + 1; where column is the attribute extracted and converted to the appropriate type. Same for a null-terminated char array next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); strcpy(column, attr.c_str()); last_pos = next_pos + 1; Instead of repeating these instructions group 9 times inside the main() , we could use functions, overloading and template to improve the code quality, maintainability and readability. First, the two versions only differs for the extracting instruction, so template<typename T> void get(const std::string& row, const std::string delim, size_t& last_pos, size_t& next_pos, T& column) { std::string attr; next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); extract(column, attr); //<-- save attribute to record last_pos = next_pos + 1; } and then we take advantage of function overloading with void extract(int& to, const std::string& from) { to = strtoul(from.c_str(), nullptr, 10); } void extract(char* to, const std::string& from) { strcpy(to, from.c_str()); } Error handling It can be archieved by catching exceptions or traditional error handling. First, we can avoid exceptions for the input stream and be sure to enable them for the output stream input_file.exceptions(std::ifstream::goodbit); output_file.exceptions(std::ofstream::failbit); For example, if a stream fails to open, it will be in the bad state. You can check it with if(!input_file || !output_file) { std::clog << \"Error opening \" << DATASET_FILENAME << \" or \" DATABASE_FILENAME << \"\\n\"; return EXIT_FAILURE; } We had to check every time if there are reading errors after calling getline() with 3 if(input_file.fail()) { std::clog << \"Error occurred while reading from \" << DATASET_FILENAME << \"\\n\"; return EXIT_FAILURE; } Finally, write instructions to the output file are near so we can group them inside a try block try { output_file.write(reinterpret_cast<char*>(&record), sizeof(Record)); count += 1; if( (count % records_per_page) == 0) { output_file.write(overhead_data, overhead_byte_length); } } catch(std::ofstream::failure& e) { std::clog << \"Error occurred while writing on \" << DATABASE_FILENAME << \": \" << e.what() << \"\\n\"; return EXIT_FAILURE; } Anyway, this is not the best way to use exceptions. An exception is handled in order to recover from run-time errors and with the RAII technique 4 . Results Well, not very encouraging. We obtained a file of 25.091.696 byte aganist the original one of 10.395.430 byte: an increment of +141,37%. A bit too much... We sacrificed a lot of memory space with access speed. This will be discussed later. Be careful that the last page is partial: this could be a problem when it's time to read it because the size is less than PAGE_SIZE = 4096 byte. https://en.cppreference.com/w/cpp/string/basic_string/find \u21a9 https://en.cppreference.com/w/cpp/string/basic_string/substr \u21a9 https://en.cppreference.com/w/cpp/string/basic_string/getline \u21a9 Bjarne Stroustrup. The C++ Programming Language, 4th edition, \u00a713.1 \u21a9","title":"Part 2"},{"location":"part2/#part-2","text":"We need to take the csv dataset cities.csv as input and generate a binary file. We should call it databse.bin . Next, we will use this one with index-gen executable in order to generate an index binary file index.bin . For now, it's bettere to save to a file all those filenames. Let's call it params.h : #define DATASET_FILENAME \"../cities.csv\" #define DATABASE_FILENAME \"../database.bin\" #define INDEX_FILENAME \"../index.bin\" I am using CMake with Visual Studio Code. In doing so, it will be faster to compile and execute the code during the development.","title":"Part 2"},{"location":"part2/#organization-of-database-file","text":"We would read all records from cities.csv and write them to database.bin like a simply collection of Record s. But we want to take advantage of paging to improve performance. This will introduce just a bit of complexity. First, add to params.h the line #define PAGE_SIZE 4096 This is usually the page size in most systems. So, if sizeof(Record) = 176 bytes, and PAGE_SIZE is 4096, each paga will contains PAGE_SIZE / sizeof(Record) = 23 records with PAGE_SIZE % sizeof(Record) = 48 overhead byte remaining.","title":"Organization of database file"},{"location":"part2/#csv2bin","text":"The source csv2bin.cpp simply reads each row from DATASET_FILENAME associated input stream, save it in a Record variable and append it to DATABASE_FILENAME associated output stream. Each 23 reads we have to write 48 byte to finish a page so const unsigned records_per_page{ PAGE_SIZE / sizeof(Record) }; //23 const unsigned overhead_byte_length{ PAGE_SIZE % sizeof(Record) }; //48 byte const char overhead_data[overhead_byte_length]{0}; size_t count{0}; //Record reads counter while(!input_file.eof()) { std::getline(input_file, row, '\\n'); //extracts a row /** * Save a row/line to Record variable */ output_file.write(reinterpret_cast<char*>(&record), sizeof(Record)); count += 1; if( (count % records_per_page) == 0) { //each 23 reads output_file.write(overhead_data, overhead_byte_length); //finish a page } } //end while-loop And this is the core idea.","title":"csv2bin"},{"location":"part2/#extracting-attributes-from-a-row","text":"Each column is separate by a ; delimiter and we are storing an entire row to a std::string class type. So let's take advantage from the C++ standard library using find() 1 and substr() 2 methods. We could do something like this for int type std::string attr; next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); column = strtoul(attr.c_str(), nullptr, 10); last_pos = next_pos + 1; where column is the attribute extracted and converted to the appropriate type. Same for a null-terminated char array next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); strcpy(column, attr.c_str()); last_pos = next_pos + 1; Instead of repeating these instructions group 9 times inside the main() , we could use functions, overloading and template to improve the code quality, maintainability and readability. First, the two versions only differs for the extracting instruction, so template<typename T> void get(const std::string& row, const std::string delim, size_t& last_pos, size_t& next_pos, T& column) { std::string attr; next_pos = row.find(delim, last_pos); attr = row.substr(last_pos, next_pos-last_pos); extract(column, attr); //<-- save attribute to record last_pos = next_pos + 1; } and then we take advantage of function overloading with void extract(int& to, const std::string& from) { to = strtoul(from.c_str(), nullptr, 10); } void extract(char* to, const std::string& from) { strcpy(to, from.c_str()); }","title":"Extracting attributes from a row"},{"location":"part2/#error-handling","text":"It can be archieved by catching exceptions or traditional error handling. First, we can avoid exceptions for the input stream and be sure to enable them for the output stream input_file.exceptions(std::ifstream::goodbit); output_file.exceptions(std::ofstream::failbit); For example, if a stream fails to open, it will be in the bad state. You can check it with if(!input_file || !output_file) { std::clog << \"Error opening \" << DATASET_FILENAME << \" or \" DATABASE_FILENAME << \"\\n\"; return EXIT_FAILURE; } We had to check every time if there are reading errors after calling getline() with 3 if(input_file.fail()) { std::clog << \"Error occurred while reading from \" << DATASET_FILENAME << \"\\n\"; return EXIT_FAILURE; } Finally, write instructions to the output file are near so we can group them inside a try block try { output_file.write(reinterpret_cast<char*>(&record), sizeof(Record)); count += 1; if( (count % records_per_page) == 0) { output_file.write(overhead_data, overhead_byte_length); } } catch(std::ofstream::failure& e) { std::clog << \"Error occurred while writing on \" << DATABASE_FILENAME << \": \" << e.what() << \"\\n\"; return EXIT_FAILURE; } Anyway, this is not the best way to use exceptions. An exception is handled in order to recover from run-time errors and with the RAII technique 4 .","title":"Error handling"},{"location":"part2/#results","text":"Well, not very encouraging. We obtained a file of 25.091.696 byte aganist the original one of 10.395.430 byte: an increment of +141,37%. A bit too much... We sacrificed a lot of memory space with access speed. This will be discussed later. Be careful that the last page is partial: this could be a problem when it's time to read it because the size is less than PAGE_SIZE = 4096 byte. https://en.cppreference.com/w/cpp/string/basic_string/find \u21a9 https://en.cppreference.com/w/cpp/string/basic_string/substr \u21a9 https://en.cppreference.com/w/cpp/string/basic_string/getline \u21a9 Bjarne Stroustrup. The C++ Programming Language, 4th edition, \u00a713.1 \u21a9","title":"Results"},{"location":"part3/","text":"Part 3 Records stored in database.bin are not sorted by any column. So, it is just an agglomerate of Records with some padding, grouped by pages. Not very useful for our purpose. We need to to select a proper column as a primary key, let's say A , in order to have an A -sorted data file. Next step is to generate an index file so it can be used to access to index entry: it is an ordered pair (k i , p i ) where k i is the minimal value inside the page p i , witch contains a certain number of Records. Since a Record is 176 byte long, and a page holds 23 Records, the key k i of page p i belonging to the Record with the A-column value is less than any other inside the same page and the next ones. You can found the code described in this section in index-gen.cpp . Getting, sorting and saving data We need to Fetch all Records from the database.bin to a vector Sort them by a column Write the sorted vector to database.bin Fortunally, we can take advantage of std::vector and std::sort() provided by the C++ Standard Library. load_data() The function declaration is std::vector<Record> load_data(const std::string& filename) It will set up a std::vector with all Records loaded from the database.bin and will returns it. Quite simple. We need to know how many Records are presente inside the database. So we will open the file in binary / read mode with the std::ios_base::ate flag. Then, the tellg() method will returns the file byte size, say db_size . Since PAGE_SIZE = 4096 and sizeof_record = sizeof(Record) with records_per_page = PAGE_SIZE / sizeof_record provide the number of records in a single page with page_count = db_size / PAGE_SIZE provide get the number of pages page_count * records_per_page provide the number of records in the database with remaining_bytes = db_size % PAGE_SIZE provide the remaing byte of the partial page remaining_bytes / sizeof_record provide the number of records in the last page So the sum of records is unsigned int record_count{ (page_count * records_per_page) + (remaining_bytes / sizeof_record) }; We will pass this value to std::vector constructor. About narrowing conversion We want to avoid as much as possible implicit conversions that can lead to not only loss of data but also values falsification like assigning a -1 value to a unsigned variable by mistake. Compiler can warn you when and where a narrowing conversion occour and we had to take a closer look in those points. Bjarne debate abount implicit type conversion in The C++ Programming Language, \u0300\u00a710.5. const std::streamoff db_size{file.tellg()}; if(db_size == -1) { std::clog << \"Error getting file size of \" << filename << \"\\n\"; exit(EXIT_FAILURE); } tellg() returns a std::streampos, aka long long, so the return value will be stored in a same type variable. After checking if no failure accur, we can assume that is a positive value and go on. const unsigned int sizeof_record{sizeof(Record)}; Compiler know that sizeof(Record) fits inside an unsigned int so it won't issue any narrowing conversion message. const unsigned int records_per_page{ PAGE_SIZE / sizeof_record }; Almost the same for this. const unsigned int page_count( db_size / PAGE_SIZE ); const unsigned remaining_bytes( db_size % PAGE_SIZE ); In order to avoid narrowing conversion messages by compiler, we had to initialize the two variables without the {}-initializer notation. Fortunally, we already checked if db_size is positive so we can safety do this. The remainder of the function is quite self-explanatory. //TODO: talk about move semantics Sorting and writing data We can take advantage of the standard library for sorting data with lambda expression 1 . In the main() function, std::vector<Record> database{load_data(DATABASE_FILENAME)}; std::sort(database.begin(), database.end(), [](const Record& a, const Record& b) { return a.id < b.id; } ); Writing data should not be a problematic. Generating index file Same for the database, we will generate a std::vector of index entries e write it to a file index.bin . To represent a pair of (key, page), I decide to use the std::pair from the standard library. This class is a template, so we can save time putting using index_entry_t = std::pair<int, int>; in params.h . Setting up the index should not be so hard std::vector<index_entry_t> index(num_of_pages); for(int page = 0; page < num_of_pages; page += 1) { index[page].first = database[page*records_per_page].id; index[page].second = page; } We take the first Record of each page and build the index entry vector jumping records_per_page == 23 Records each time. Last, we save the vector to a file like we already did before. Bjarne Stroutrup. The C++ Programming Language, \u0300\u00a711.4 \u21a9","title":"Part 3"},{"location":"part3/#part-3","text":"Records stored in database.bin are not sorted by any column. So, it is just an agglomerate of Records with some padding, grouped by pages. Not very useful for our purpose. We need to to select a proper column as a primary key, let's say A , in order to have an A -sorted data file. Next step is to generate an index file so it can be used to access to index entry: it is an ordered pair (k i , p i ) where k i is the minimal value inside the page p i , witch contains a certain number of Records. Since a Record is 176 byte long, and a page holds 23 Records, the key k i of page p i belonging to the Record with the A-column value is less than any other inside the same page and the next ones. You can found the code described in this section in index-gen.cpp .","title":"Part 3"},{"location":"part3/#getting-sorting-and-saving-data","text":"We need to Fetch all Records from the database.bin to a vector Sort them by a column Write the sorted vector to database.bin Fortunally, we can take advantage of std::vector and std::sort() provided by the C++ Standard Library.","title":"Getting, sorting and saving data"},{"location":"part3/#load_data","text":"The function declaration is std::vector<Record> load_data(const std::string& filename) It will set up a std::vector with all Records loaded from the database.bin and will returns it. Quite simple. We need to know how many Records are presente inside the database. So we will open the file in binary / read mode with the std::ios_base::ate flag. Then, the tellg() method will returns the file byte size, say db_size . Since PAGE_SIZE = 4096 and sizeof_record = sizeof(Record) with records_per_page = PAGE_SIZE / sizeof_record provide the number of records in a single page with page_count = db_size / PAGE_SIZE provide get the number of pages page_count * records_per_page provide the number of records in the database with remaining_bytes = db_size % PAGE_SIZE provide the remaing byte of the partial page remaining_bytes / sizeof_record provide the number of records in the last page So the sum of records is unsigned int record_count{ (page_count * records_per_page) + (remaining_bytes / sizeof_record) }; We will pass this value to std::vector constructor.","title":"load_data()"},{"location":"part3/#about-narrowing-conversion","text":"We want to avoid as much as possible implicit conversions that can lead to not only loss of data but also values falsification like assigning a -1 value to a unsigned variable by mistake. Compiler can warn you when and where a narrowing conversion occour and we had to take a closer look in those points. Bjarne debate abount implicit type conversion in The C++ Programming Language, \u0300\u00a710.5. const std::streamoff db_size{file.tellg()}; if(db_size == -1) { std::clog << \"Error getting file size of \" << filename << \"\\n\"; exit(EXIT_FAILURE); } tellg() returns a std::streampos, aka long long, so the return value will be stored in a same type variable. After checking if no failure accur, we can assume that is a positive value and go on. const unsigned int sizeof_record{sizeof(Record)}; Compiler know that sizeof(Record) fits inside an unsigned int so it won't issue any narrowing conversion message. const unsigned int records_per_page{ PAGE_SIZE / sizeof_record }; Almost the same for this. const unsigned int page_count( db_size / PAGE_SIZE ); const unsigned remaining_bytes( db_size % PAGE_SIZE ); In order to avoid narrowing conversion messages by compiler, we had to initialize the two variables without the {}-initializer notation. Fortunally, we already checked if db_size is positive so we can safety do this. The remainder of the function is quite self-explanatory. //TODO: talk about move semantics","title":"About narrowing conversion"},{"location":"part3/#sorting-and-writing-data","text":"We can take advantage of the standard library for sorting data with lambda expression 1 . In the main() function, std::vector<Record> database{load_data(DATABASE_FILENAME)}; std::sort(database.begin(), database.end(), [](const Record& a, const Record& b) { return a.id < b.id; } ); Writing data should not be a problematic.","title":"Sorting and writing data"},{"location":"part3/#generating-index-file","text":"Same for the database, we will generate a std::vector of index entries e write it to a file index.bin . To represent a pair of (key, page), I decide to use the std::pair from the standard library. This class is a template, so we can save time putting using index_entry_t = std::pair<int, int>; in params.h . Setting up the index should not be so hard std::vector<index_entry_t> index(num_of_pages); for(int page = 0; page < num_of_pages; page += 1) { index[page].first = database[page*records_per_page].id; index[page].second = page; } We take the first Record of each page and build the index entry vector jumping records_per_page == 23 Records each time. Last, we save the vector to a file like we already did before. Bjarne Stroutrup. The C++ Programming Language, \u0300\u00a711.4 \u21a9","title":"Generating index file"}]}